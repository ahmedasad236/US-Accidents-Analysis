{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, avg, to_timestamp, hour, date_format, month, year, when, mean\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "key_filepath = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/07 11:32:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"US Accidents\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\",key_filepath  ) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read Parquet files from GCS\n",
    "parquet_files = [\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2016/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2017/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2018/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2019/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2020/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2021/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2022/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2023/933c14c388864f19a17c514e311a69b1-0.parquet\"\n",
    "                ]\n",
    "\n",
    "df_list = [spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(file) for file in parquet_files]\n",
    "\n",
    "merged_df = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    merged_df = merged_df.unionAll(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/30 23:00:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------------------+-------------------+-----------------+------------------+------------+--------------------+--------------------+------------+----------+-----+-------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+\n",
      "|  ID| Source|Severity|         Start_Time|           End_Time|        Start_Lat|         Start_Lng|Distance(mi)|         Description|              Street|        City|    County|State|Country|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Weather_Condition|Crossing|Railway|Station|Traffic_Calming|Traffic_Signal|Sunrise_Sunset|\n",
      "+----+-------+--------+-------------------+-------------------+-----------------+------------------+------------+--------------------+--------------------+------------+----------+-----+-------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+\n",
      "| A-1|Source2|       3|2016-02-08 05:46:00|2016-02-08 11:00:00|        39.865147|        -84.058723|        0.01|Right lane blocke...|              I-70 E|      Dayton|Montgomery|   OH|     US|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|       Light Rain|   false|  false|  false|          false|         false|         Night|\n",
      "| A-2|Source2|       2|2016-02-08 06:07:59|2016-02-08 06:37:59|39.92805900000001|        -82.831184|        0.01|Accident on Brice...|            Brice Rd|Reynoldsburg|  Franklin|   OH|     US|          37.9|         null|      100.0|       29.65|          10.0|          Calm|           null|       Light Rain|   false|  false|  false|          false|         false|         Night|\n",
      "| A-3|Source2|       2|2016-02-08 06:49:27|2016-02-08 07:19:27|        39.063148|        -84.032608|        0.01|Accident on OH-32...|      State Route 32|Williamsburg|  Clermont|   OH|     US|          36.0|         33.3|      100.0|       29.67|          10.0|            SW|            3.5|         Overcast|   false|  false|  false|          false|          true|         Night|\n",
      "| A-4|Source2|       3|2016-02-08 07:23:34|2016-02-08 07:53:34|        39.747753|-84.20558199999998|        0.01|Accident on I-75 ...|              I-75 S|      Dayton|Montgomery|   OH|     US|          35.1|         31.0|       96.0|       29.64|           9.0|            SW|            4.6|    Mostly Cloudy|   false|  false|  false|          false|         false|         Night|\n",
      "| A-5|Source2|       2|2016-02-08 07:39:07|2016-02-08 08:09:07|        39.627781|        -84.188354|        0.01|Accident on McEwe...|Miamisburg Center...|      Dayton|Montgomery|   OH|     US|          36.0|         33.3|       89.0|       29.65|           6.0|            SW|            3.5|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|\n",
      "| A-6|Source2|       3|2016-02-08 07:44:26|2016-02-08 08:14:26|         40.10059|-82.92519399999998|        0.01|Accident on I-270...|      Westerville Rd| Westerville|  Franklin|   OH|     US|          37.9|         35.5|       97.0|       29.63|           7.0|           SSW|            3.5|       Light Rain|   false|  false|  false|          false|         false|           Day|\n",
      "| A-7|Source2|       2|2016-02-08 07:59:35|2016-02-08 08:29:35|        39.758274|-84.23050699999997|         0.0|Accident on Oakri...|      N Woodward Ave|      Dayton|Montgomery|   OH|     US|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "| A-8|Source2|       3|2016-02-08 07:59:58|2016-02-08 08:29:58|        39.770382|        -84.194901|        0.01|Accident on I-75 ...|           N Main St|      Dayton|Montgomery|   OH|     US|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "| A-9|Source2|       2|2016-02-08 08:00:40|2016-02-08 08:30:40|        39.778061|        -84.172005|         0.0|Accident on Notre...|      Notre Dame Ave|      Dayton|Montgomery|   OH|     US|          33.3|         null|       99.0|       29.67|           5.0|            SW|            1.2|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "|A-10|Source2|       3|2016-02-08 08:10:04|2016-02-08 08:40:04|         40.10059|-82.92519399999998|        0.01|Right hand should...|      Westerville Rd| Westerville|  Franklin|   OH|     US|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|\n",
      "|A-11|Source2|       3|2016-02-08 08:14:42|2016-02-08 08:44:42|        39.952812|        -83.119293|        0.01|Accident on I-270...|         Outerbelt S|    Columbus|  Franklin|   OH|     US|          35.6|         30.7|       93.0|       29.64|           5.0|           WNW|            5.8|             Rain|    true|  false|  false|          false|         false|           Day|\n",
      "|A-12|Source2|       3|2016-02-08 08:21:27|2016-02-08 08:51:27|        39.932709|         -82.83091|        0.01|One lane blocked ...|              I-70 E|Reynoldsburg|  Franklin|   OH|     US|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|\n",
      "|A-13|Source2|       2|2016-02-08 08:36:34|2016-02-08 09:06:34|        39.737633|-84.14993299999998|         0.0|Accident on Rever...|      Watervliet Ave|      Dayton|Montgomery|   OH|     US|          33.8|         null|      100.0|       29.63|           3.0|            SW|            2.3|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "|A-14|Source2|       2|2016-02-08 08:37:07|2016-02-08 09:07:07|         39.79076|        -84.241547|        0.01|Accident on Salem...|           Salem Ave|      Dayton|Montgomery|   OH|     US|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|\n",
      "|A-15|Source2|       2|2016-02-08 08:39:43|2016-02-08 09:09:43|        39.972038|        -82.913521|        0.01|Accident on OH-16...|          E Broad St|    Columbus|  Franklin|   OH|     US|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|          true|           Day|\n",
      "|A-16|Source2|       2|2016-02-08 08:43:20|2016-02-08 09:13:20|        39.745888|         -84.17041|        0.01|Accident on Wayne...|         Glencoe Ave|      Dayton|Montgomery|   OH|     US|          33.8|         null|      100.0|       29.63|           3.0|            SW|            2.3|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "|A-17|Source2|       2|2016-02-08 08:53:17|2016-02-08 09:23:17|        39.748329|        -84.224007|        0.01|Accident on James...|S James H McGee Blvd|      Dayton|Montgomery|   OH|     US|          35.6|         null|       99.0|       29.65|           7.0|           WSW|            2.3|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "|A-18|Source2|       2|2016-02-08 09:24:37|2016-02-08 09:54:37|        39.752174|        -84.239952|         0.0|Accident on Delph...|         Delphos Ave|      Dayton|Montgomery|   OH|     US|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "|A-19|Source2|       2|2016-02-08 09:25:17|2016-02-08 09:55:17|        39.740669|        -84.184135|        0.01|Accident on Stewa...|          Rubicon St|      Dayton|Montgomery|   OH|     US|          37.4|         32.1|       93.0|       29.63|          10.0|           WSW|            6.9|         Overcast|    true|  false|  false|          false|          true|           Day|\n",
      "|A-20|Source2|       2|2016-02-08 09:35:35|2016-02-08 10:05:35|        39.790703|        -84.244461|        0.01|Accident on Hillc...|     W Hillcrest Ave|      Dayton|Montgomery|   OH|     US|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "+----+-------+--------+-------------------+-------------------+-----------------+------------------+------------+--------------------+--------------------+------------+----------+-----+-------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "\n",
    "### Steps:\n",
    "> Removing unimportnant columns\n",
    "\n",
    "> Renaming the columns\n",
    "\n",
    "> Convert the values of the columns into easy handled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Convert \"Start_Time\" and \"End_Time\" columns to datetime format\n",
    "merged_df = merged_df.withColumn(\"Start_Time\", to_timestamp(merged_df[\"Start_Time\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "merged_df = merged_df.withColumn(\"End_Time\", to_timestamp(merged_df[\"End_Time\"], \"yyyy-MM-dd HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert \"Severity\" column from string to integer\n",
    "merged_df = merged_df.withColumn(\"Severity\", merged_df[\"Severity\"].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If the rows that have at least one null value remove them, and if they are large number \n",
    "# replace the null with the average of its column\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5656839"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have dropped about 2 millions records because of nulls :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called Duration = End_Time - Start_Time\n",
    "# To indicate the duration of the accident\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "# Convert \"Start_Time\" and \"End_Time\" columns to Unix timestamp (seconds since the epoch)\n",
    "merged_df = merged_df.withColumn(\"Start_Time_unix\", unix_timestamp(\"Start_Time\"))\n",
    "merged_df = merged_df.withColumn(\"End_Time_unix\", unix_timestamp(\"End_Time\"))\n",
    "\n",
    "# Calculate the duration (in seconds)\n",
    "merged_df = merged_df.withColumn(\"Duration\", col(\"End_Time_unix\") - col(\"Start_Time_unix\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for start_time\n",
    "merged_df = merged_df.withColumn('Hour_Of_Acc', hour(merged_df['Start_Time']))\n",
    "merged_df = merged_df.withColumn('Day_Of_Acc', date_format(merged_df['Start_Time'], 'EEEE'))\n",
    "merged_df = merged_df.withColumn('Month_Of_Acc', month(merged_df['Start_Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'when' function to assign temperature categories based on the bins\n",
    "temperature_category_column = when(merged_df['Temperature(F)'] <= 50, 'Cold') \\\n",
    "    .when((merged_df['Temperature(F)'] > 50) & (merged_df['Temperature(F)'] <= 75), 'Moderate') \\\n",
    "    .otherwise('Warm')\n",
    "\n",
    "# Add the temperature category column to the DataFrame\n",
    "merged_df = merged_df.withColumn('Temperature_Category', temperature_category_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Distance(mi)\" column from miles to meters\n",
    "merged_df = merged_df.withColumn(\"Distance(m)\", col(\"Distance(mi)\") * 1609.34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary or redundant columns\n",
    "\n",
    "# List of column names to drop\n",
    "columns_to_drop = ['Country', 'Start_Time', 'End_Time', 'Distance(mi)', 'Start_Time_unix', 'End_Time_unix']\n",
    "\n",
    "# Drop the unimportant columns\n",
    "merged_df = merged_df.drop(*columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---------+------------------+--------------------+--------------------+------------+----------+-----+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+--------+-----------+----------+------------+--------------------+-----------+\n",
      "|  ID| Source|Severity|Start_Lat|         Start_Lng|         Description|              Street|        City|    County|State|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Weather_Condition|Crossing|Railway|Station|Traffic_Calming|Traffic_Signal|Sunrise_Sunset|Duration|Hour_Of_Acc|Day_Of_Acc|Month_Of_Acc|Temperature_Category|Distance(m)|\n",
      "+----+-------+--------+---------+------------------+--------------------+--------------------+------------+----------+-----+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+--------+-----------+----------+------------+--------------------+-----------+\n",
      "| A-3|Source2|       2|39.063148|        -84.032608|Accident on OH-32...|      State Route 32|Williamsburg|  Clermont|   OH|          36.0|         33.3|      100.0|       29.67|          10.0|            SW|            3.5|         Overcast|   false|  false|  false|          false|          true|         Night|    1800|          6|    Monday|           2|                Cold|    16.0934|\n",
      "| A-4|Source2|       3|39.747753|-84.20558199999998|Accident on I-75 ...|              I-75 S|      Dayton|Montgomery|   OH|          35.1|         31.0|       96.0|       29.64|           9.0|            SW|            4.6|    Mostly Cloudy|   false|  false|  false|          false|         false|         Night|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "| A-5|Source2|       2|39.627781|        -84.188354|Accident on McEwe...|Miamisburg Center...|      Dayton|Montgomery|   OH|          36.0|         33.3|       89.0|       29.65|           6.0|            SW|            3.5|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "| A-6|Source2|       3| 40.10059|-82.92519399999998|Accident on I-270...|      Westerville Rd| Westerville|  Franklin|   OH|          37.9|         35.5|       97.0|       29.63|           7.0|           SSW|            3.5|       Light Rain|   false|  false|  false|          false|         false|           Day|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "| A-7|Source2|       2|39.758274|-84.23050699999997|Accident on Oakri...|      N Woodward Ave|      Dayton|Montgomery|   OH|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|    1800|          7|    Monday|           2|                Cold|        0.0|\n",
      "| A-8|Source2|       3|39.770382|        -84.194901|Accident on I-75 ...|           N Main St|      Dayton|Montgomery|   OH|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "|A-10|Source2|       3| 40.10059|-82.92519399999998|Right hand should...|      Westerville Rd| Westerville|  Franklin|   OH|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-11|Source2|       3|39.952812|        -83.119293|Accident on I-270...|         Outerbelt S|    Columbus|  Franklin|   OH|          35.6|         30.7|       93.0|       29.64|           5.0|           WNW|            5.8|             Rain|    true|  false|  false|          false|         false|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-12|Source2|       3|39.932709|         -82.83091|One lane blocked ...|              I-70 E|Reynoldsburg|  Franklin|   OH|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-14|Source2|       2| 39.79076|        -84.241547|Accident on Salem...|           Salem Ave|      Dayton|Montgomery|   OH|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-15|Source2|       2|39.972038|        -82.913521|Accident on OH-16...|          E Broad St|    Columbus|  Franklin|   OH|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|          true|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-18|Source2|       2|39.752174|        -84.239952|Accident on Delph...|         Delphos Ave|      Dayton|Montgomery|   OH|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|    1800|          9|    Monday|           2|                Cold|        0.0|\n",
      "|A-19|Source2|       2|39.740669|        -84.184135|Accident on Stewa...|          Rubicon St|      Dayton|Montgomery|   OH|          37.4|         32.1|       93.0|       29.63|          10.0|           WSW|            6.9|         Overcast|    true|  false|  false|          false|          true|           Day|    1800|          9|    Monday|           2|                Cold|    16.0934|\n",
      "|A-20|Source2|       2|39.790703|        -84.244461|Accident on Hillc...|     W Hillcrest Ave|      Dayton|Montgomery|   OH|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|    1800|          9|    Monday|           2|                Cold|    16.0934|\n",
      "|A-21|Source2|       2|40.052509|-82.88233199999998|Accident on Brook...|        Brookhill Dr|    Columbus|  Franklin|   OH|          33.8|         29.6|      100.0|       29.62|           2.0|           NNW|            4.6|       Light Snow|   false|  false|  false|          false|         false|           Day|    1800|         10|    Monday|           2|                Cold|        0.0|\n",
      "|A-22|Source2|       2|39.773346|        -84.224686|Accident on Princ...|        Princeton Dr|      Dayton|Montgomery|   OH|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|    1800|         10|    Monday|           2|                Cold|        0.0|\n",
      "|A-23|Source2|       2|39.628288|        -84.226151|Accident on OH-74...|     Springboro Pike|  Miamisburg|Montgomery|   OH|          35.1|         28.6|       89.0|       29.65|           6.0|           WSW|            8.1|         Overcast|   false|  false|  false|          false|          true|           Day|    1800|         11|    Monday|           2|                Cold|    16.0934|\n",
      "|A-24|Source2|       3|40.023487|        -82.994888|Accident on I-71 ...|         North Fwy S|    Columbus|  Franklin|   OH|          37.0|         32.4|       96.0|       29.63|           8.0|          West|            5.8|         Overcast|   false|  false|  false|          false|         false|           Day|    2700|         12|    Monday|           2|                Cold|    16.0934|\n",
      "|A-25|Source2|       2|39.761379|-84.25921600000002|Accident on Hoove...|          Hoover Ave|      Dayton|Montgomery|   OH|          37.9|         30.9|       76.0|       29.62|          10.0|           WNW|           10.4|         Overcast|   false|  false|  false|          false|          true|           Day|    2700|         12|    Monday|           2|                Cold|    16.0934|\n",
      "|A-26|Source2|       2|40.158024|        -82.641762|Accident on Count...|   Sportsman Club Rd|   Johnstown|   Licking|   OH|          37.9|         34.4|      100.0|        29.6|           5.0|          West|            4.6|       Light Snow|   false|  false|  false|          false|         false|           Day|    1800|         12|    Monday|           2|                Cold|  2124.3288|\n",
      "+----+-------+--------+---------+------------------+--------------------+--------------------+------------+----------+-----+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+--------+-----------+----------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Source',\n",
       " 'Severity',\n",
       " 'Start_Time',\n",
       " 'End_Time',\n",
       " 'Start_Lat',\n",
       " 'Start_Lng',\n",
       " 'Description',\n",
       " 'Street',\n",
       " 'City',\n",
       " 'County',\n",
       " 'State',\n",
       " 'Temperature(F)',\n",
       " 'Wind_Chill(F)',\n",
       " 'Humidity(%)',\n",
       " 'Pressure(in)',\n",
       " 'Visibility(mi)',\n",
       " 'Wind_Direction',\n",
       " 'Wind_Speed(mph)',\n",
       " 'Weather_Condition',\n",
       " 'Crossing',\n",
       " 'Railway',\n",
       " 'Station',\n",
       " 'Traffic_Calming',\n",
       " 'Traffic_Signal',\n",
       " 'Sunrise_Sunset',\n",
       " 'Distance(m)',\n",
       " 'Start_Time_unix',\n",
       " 'End_Time_unix',\n",
       " 'Duration']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics\n",
    "\n",
    "1. [Regression]➔ Predicting accident Duration as indicator of impact on\n",
    "traffic flow.\n",
    "\n",
    "2. [classification]➔ Predicting the severity of an accident based on the\n",
    "factors involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the EDA phase we will use the following features in our model:\n",
    "- Start_Lat\n",
    "- Start_Lng\n",
    "- Source\n",
    "- Duration\n",
    "- Hour_Of_Acc\n",
    "- Day_Of_Acc\n",
    "- Month_Of_Acc\n",
    "- Temperature\n",
    "- Distance\n",
    "- State\n",
    "- Crossing\n",
    "- Railway\n",
    "- Traffic_Calming\n",
    "- Traffic_Signal\n",
    "\n",
    "### Note: \n",
    "\n",
    "We execluded most of weather conditions as it does not affect on the severity as follows:  \n",
    "\n",
    "Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression_mr import linear_regression, predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "|Start_Lat|         Start_Lng| Source|Duration|Hour_Of_Acc|Day_Of_Acc|Month_Of_Acc|Temperature(F)|Distance(m)|State|        City|Crossing|Railway|Traffic_Calming|Traffic_Signal|Severity|\n",
      "+---------+------------------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "|39.063148|        -84.032608|Source2|    1800|          6|    Monday|           2|          36.0|    16.0934|   OH|Williamsburg|   false|  false|          false|          true|       2|\n",
      "|39.747753|-84.20558199999998|Source2|    1800|          7|    Monday|           2|          35.1|    16.0934|   OH|      Dayton|   false|  false|          false|         false|       3|\n",
      "|39.627781|        -84.188354|Source2|    1800|          7|    Monday|           2|          36.0|    16.0934|   OH|      Dayton|   false|  false|          false|          true|       2|\n",
      "| 40.10059|-82.92519399999998|Source2|    1800|          7|    Monday|           2|          37.9|    16.0934|   OH| Westerville|   false|  false|          false|         false|       3|\n",
      "|39.758274|-84.23050699999997|Source2|    1800|          7|    Monday|           2|          34.0|        0.0|   OH|      Dayton|   false|  false|          false|         false|       2|\n",
      "|39.770382|        -84.194901|Source2|    1800|          7|    Monday|           2|          34.0|    16.0934|   OH|      Dayton|   false|  false|          false|         false|       3|\n",
      "| 40.10059|-82.92519399999998|Source2|    1800|          8|    Monday|           2|          37.4|    16.0934|   OH| Westerville|   false|  false|          false|         false|       3|\n",
      "|39.952812|        -83.119293|Source2|    1800|          8|    Monday|           2|          35.6|    16.0934|   OH|    Columbus|    true|  false|          false|         false|       3|\n",
      "|39.932709|         -82.83091|Source2|    1800|          8|    Monday|           2|          37.4|    16.0934|   OH|Reynoldsburg|   false|  false|          false|         false|       3|\n",
      "| 39.79076|        -84.241547|Source2|    1800|          8|    Monday|           2|          36.0|    16.0934|   OH|      Dayton|   false|  false|          false|          true|       2|\n",
      "|39.972038|        -82.913521|Source2|    1800|          8|    Monday|           2|          37.4|    16.0934|   OH|    Columbus|   false|  false|          false|          true|       2|\n",
      "|39.752174|        -84.239952|Source2|    1800|          9|    Monday|           2|          36.0|        0.0|   OH|      Dayton|   false|  false|          false|         false|       2|\n",
      "|39.740669|        -84.184135|Source2|    1800|          9|    Monday|           2|          37.4|    16.0934|   OH|      Dayton|    true|  false|          false|          true|       2|\n",
      "|39.790703|        -84.244461|Source2|    1800|          9|    Monday|           2|          36.0|    16.0934|   OH|      Dayton|   false|  false|          false|         false|       2|\n",
      "|40.052509|-82.88233199999998|Source2|    1800|         10|    Monday|           2|          33.8|        0.0|   OH|    Columbus|   false|  false|          false|         false|       2|\n",
      "|39.773346|        -84.224686|Source2|    1800|         10|    Monday|           2|          36.0|        0.0|   OH|      Dayton|   false|  false|          false|         false|       2|\n",
      "|39.628288|        -84.226151|Source2|    1800|         11|    Monday|           2|          35.1|    16.0934|   OH|  Miamisburg|   false|  false|          false|          true|       2|\n",
      "|40.023487|        -82.994888|Source2|    2700|         12|    Monday|           2|          37.0|    16.0934|   OH|    Columbus|   false|  false|          false|         false|       3|\n",
      "|39.761379|-84.25921600000002|Source2|    2700|         12|    Monday|           2|          37.9|    16.0934|   OH|      Dayton|   false|  false|          false|          true|       2|\n",
      "|40.158024|        -82.641762|Source2|    1800|         12|    Monday|           2|          37.9|  2124.3288|   OH|   Johnstown|   false|  false|          false|         false|       2|\n",
      "+---------+------------------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_to_keep = [\n",
    "    \"Start_Lat\", \"Start_Lng\",  'Source', \"Duration\", \"Hour_Of_Acc\", \"Day_Of_Acc\",\n",
    "    \"Month_Of_Acc\", \"Temperature(F)\", \"Distance(m)\", \"State\", \"City\", \"Crossing\",\n",
    "    \"Railway\", \"Traffic_Calming\", \"Traffic_Signal\", \"Severity\"\n",
    "]\n",
    "\n",
    "# Selecting only the desired columns\n",
    "lr_data_df = merged_df.select(columns_to_keep)\n",
    "\n",
    "# Showing the resulting DataFrame\n",
    "lr_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o575.fit.\n: org.apache.spark.SparkException: Input column Day_Of_Week does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m indexers \u001b[38;5;241m=\u001b[39m [StringIndexer(inputCol\u001b[38;5;241m=\u001b[39mcol_name, outputCol\u001b[38;5;241m=\u001b[39mcol_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m string_cols]\n\u001b[1;32m     11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39mindexers)\n\u001b[0;32m---> 12\u001b[0m new_df \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(new_df)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Convert indexed string columns to one-hot encoded vectors\u001b[39;00m\n\u001b[1;32m     15\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(dropLast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputCols\u001b[38;5;241m=\u001b[39m[col_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m string_cols], outputCols\u001b[38;5;241m=\u001b[39m[col_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_onehot\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m string_cols])\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o575.fit.\n: org.apache.spark.SparkException: Input column Day_Of_Week does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean columns to numerical values (0 or 1)\n",
    "boolean_cols = [ 'Crossing', 'Railway', 'Traffic_Calming', 'Traffic_Signal']  \n",
    "new_df = lr_data_df\n",
    "for col_name in boolean_cols:\n",
    "    new_df = new_df.withColumn(col_name, when(col(col_name), 1).otherwise(0))\n",
    "\n",
    "# Convert string columns to numerical values using StringIndexer\n",
    "string_cols = ['Source', 'City', 'State', 'Day_Of_Week']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name+\"_index\", handleInvalid=\"keep\") for col_name in string_cols]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "new_df = pipeline.fit(new_df).transform(new_df)\n",
    "\n",
    "# Convert indexed string columns to one-hot encoded vectors\n",
    "encoder = OneHotEncoder(dropLast=False, inputCols=[col_name+\"_index\" for col_name in string_cols], outputCols=[col_name+\"_onehot\" for col_name in string_cols])\n",
    "model = encoder.fit(new_df)\n",
    "new_df = model.transform(new_df)\n",
    "\n",
    "# Drop original string columns and indexed columns\n",
    "new_df = new_df.drop(*[col_name+\"_index\" for col_name in string_cols])\n",
    "new_df = new_df.drop(*string_cols)\n",
    "\n",
    "# Show the DataFrame with converted numerical values\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_iterations = 100\n",
    "weights, cost_history = linear_regression(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
