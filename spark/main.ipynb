{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, avg, to_timestamp, hour, date_format, month, year, when, mean, expr, element_at\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "key_filepath = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/07 16:33:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"US Accidents\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\",key_filepath  ) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read Parquet files from GCS\n",
    "parquet_files = [\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2016/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2017/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2018/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2019/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2020/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2021/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2022/933c14c388864f19a17c514e311a69b1-0.parquet\",\n",
    "                \"gs://us-accidents-bucket/us_accidents_data/Start_Year=2023/933c14c388864f19a17c514e311a69b1-0.parquet\"\n",
    "                ]\n",
    "\n",
    "df_list = [spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(file) for file in parquet_files]\n",
    "\n",
    "merged_df = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    merged_df = merged_df.unionAll(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/30 23:00:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------------------+-------------------+-----------------+------------------+------------+--------------------+--------------------+------------+----------+-----+-------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+\n",
      "|  ID| Source|Severity|         Start_Time|           End_Time|        Start_Lat|         Start_Lng|Distance(mi)|         Description|              Street|        City|    County|State|Country|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Weather_Condition|Crossing|Railway|Station|Traffic_Calming|Traffic_Signal|Sunrise_Sunset|\n",
      "+----+-------+--------+-------------------+-------------------+-----------------+------------------+------------+--------------------+--------------------+------------+----------+-----+-------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+\n",
      "| A-1|Source2|       3|2016-02-08 05:46:00|2016-02-08 11:00:00|        39.865147|        -84.058723|        0.01|Right lane blocke...|              I-70 E|      Dayton|Montgomery|   OH|     US|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|       Light Rain|   false|  false|  false|          false|         false|         Night|\n",
      "| A-2|Source2|       2|2016-02-08 06:07:59|2016-02-08 06:37:59|39.92805900000001|        -82.831184|        0.01|Accident on Brice...|            Brice Rd|Reynoldsburg|  Franklin|   OH|     US|          37.9|         null|      100.0|       29.65|          10.0|          Calm|           null|       Light Rain|   false|  false|  false|          false|         false|         Night|\n",
      "| A-3|Source2|       2|2016-02-08 06:49:27|2016-02-08 07:19:27|        39.063148|        -84.032608|        0.01|Accident on OH-32...|      State Route 32|Williamsburg|  Clermont|   OH|     US|          36.0|         33.3|      100.0|       29.67|          10.0|            SW|            3.5|         Overcast|   false|  false|  false|          false|          true|         Night|\n",
      "| A-4|Source2|       3|2016-02-08 07:23:34|2016-02-08 07:53:34|        39.747753|-84.20558199999998|        0.01|Accident on I-75 ...|              I-75 S|      Dayton|Montgomery|   OH|     US|          35.1|         31.0|       96.0|       29.64|           9.0|            SW|            4.6|    Mostly Cloudy|   false|  false|  false|          false|         false|         Night|\n",
      "| A-5|Source2|       2|2016-02-08 07:39:07|2016-02-08 08:09:07|        39.627781|        -84.188354|        0.01|Accident on McEwe...|Miamisburg Center...|      Dayton|Montgomery|   OH|     US|          36.0|         33.3|       89.0|       29.65|           6.0|            SW|            3.5|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|\n",
      "| A-6|Source2|       3|2016-02-08 07:44:26|2016-02-08 08:14:26|         40.10059|-82.92519399999998|        0.01|Accident on I-270...|      Westerville Rd| Westerville|  Franklin|   OH|     US|          37.9|         35.5|       97.0|       29.63|           7.0|           SSW|            3.5|       Light Rain|   false|  false|  false|          false|         false|           Day|\n",
      "| A-7|Source2|       2|2016-02-08 07:59:35|2016-02-08 08:29:35|        39.758274|-84.23050699999997|         0.0|Accident on Oakri...|      N Woodward Ave|      Dayton|Montgomery|   OH|     US|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "| A-8|Source2|       3|2016-02-08 07:59:58|2016-02-08 08:29:58|        39.770382|        -84.194901|        0.01|Accident on I-75 ...|           N Main St|      Dayton|Montgomery|   OH|     US|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "| A-9|Source2|       2|2016-02-08 08:00:40|2016-02-08 08:30:40|        39.778061|        -84.172005|         0.0|Accident on Notre...|      Notre Dame Ave|      Dayton|Montgomery|   OH|     US|          33.3|         null|       99.0|       29.67|           5.0|            SW|            1.2|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "|A-10|Source2|       3|2016-02-08 08:10:04|2016-02-08 08:40:04|         40.10059|-82.92519399999998|        0.01|Right hand should...|      Westerville Rd| Westerville|  Franklin|   OH|     US|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|\n",
      "|A-11|Source2|       3|2016-02-08 08:14:42|2016-02-08 08:44:42|        39.952812|        -83.119293|        0.01|Accident on I-270...|         Outerbelt S|    Columbus|  Franklin|   OH|     US|          35.6|         30.7|       93.0|       29.64|           5.0|           WNW|            5.8|             Rain|    true|  false|  false|          false|         false|           Day|\n",
      "|A-12|Source2|       3|2016-02-08 08:21:27|2016-02-08 08:51:27|        39.932709|         -82.83091|        0.01|One lane blocked ...|              I-70 E|Reynoldsburg|  Franklin|   OH|     US|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|\n",
      "|A-13|Source2|       2|2016-02-08 08:36:34|2016-02-08 09:06:34|        39.737633|-84.14993299999998|         0.0|Accident on Rever...|      Watervliet Ave|      Dayton|Montgomery|   OH|     US|          33.8|         null|      100.0|       29.63|           3.0|            SW|            2.3|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "|A-14|Source2|       2|2016-02-08 08:37:07|2016-02-08 09:07:07|         39.79076|        -84.241547|        0.01|Accident on Salem...|           Salem Ave|      Dayton|Montgomery|   OH|     US|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|\n",
      "|A-15|Source2|       2|2016-02-08 08:39:43|2016-02-08 09:09:43|        39.972038|        -82.913521|        0.01|Accident on OH-16...|          E Broad St|    Columbus|  Franklin|   OH|     US|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|          true|           Day|\n",
      "|A-16|Source2|       2|2016-02-08 08:43:20|2016-02-08 09:13:20|        39.745888|         -84.17041|        0.01|Accident on Wayne...|         Glencoe Ave|      Dayton|Montgomery|   OH|     US|          33.8|         null|      100.0|       29.63|           3.0|            SW|            2.3|         Overcast|   false|  false|  false|          false|         false|           Day|\n",
      "|A-17|Source2|       2|2016-02-08 08:53:17|2016-02-08 09:23:17|        39.748329|        -84.224007|        0.01|Accident on James...|S James H McGee Blvd|      Dayton|Montgomery|   OH|     US|          35.6|         null|       99.0|       29.65|           7.0|           WSW|            2.3|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "|A-18|Source2|       2|2016-02-08 09:24:37|2016-02-08 09:54:37|        39.752174|        -84.239952|         0.0|Accident on Delph...|         Delphos Ave|      Dayton|Montgomery|   OH|     US|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "|A-19|Source2|       2|2016-02-08 09:25:17|2016-02-08 09:55:17|        39.740669|        -84.184135|        0.01|Accident on Stewa...|          Rubicon St|      Dayton|Montgomery|   OH|     US|          37.4|         32.1|       93.0|       29.63|          10.0|           WSW|            6.9|         Overcast|    true|  false|  false|          false|          true|           Day|\n",
      "|A-20|Source2|       2|2016-02-08 09:35:35|2016-02-08 10:05:35|        39.790703|        -84.244461|        0.01|Accident on Hillc...|     W Hillcrest Ave|      Dayton|Montgomery|   OH|     US|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|\n",
      "+----+-------+--------+-------------------+-------------------+-----------------+------------------+------------+--------------------+--------------------+------------+----------+-----+-------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "\n",
    "### Steps:\n",
    "> Removing unimportnant columns\n",
    "\n",
    "> Renaming the columns\n",
    "\n",
    "> Convert the values of the columns into easy handled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Convert \"Start_Time\" and \"End_Time\" columns to datetime format\n",
    "merged_df = merged_df.withColumn(\"Start_Time\", to_timestamp(merged_df[\"Start_Time\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "merged_df = merged_df.withColumn(\"End_Time\", to_timestamp(merged_df[\"End_Time\"], \"yyyy-MM-dd HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert \"Severity\" column from string to integer\n",
    "merged_df = merged_df.withColumn(\"Severity\", merged_df[\"Severity\"].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If the rows that have at least one null value remove them, and if they are large number \n",
    "# replace the null with the average of its column\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5656839"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have dropped about 2 millions records because of nulls :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called Duration = End_Time - Start_Time\n",
    "# To indicate the duration of the accident\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "# Convert \"Start_Time\" and \"End_Time\" columns to Unix timestamp (seconds since the epoch)\n",
    "merged_df = merged_df.withColumn(\"Start_Time_unix\", unix_timestamp(\"Start_Time\"))\n",
    "merged_df = merged_df.withColumn(\"End_Time_unix\", unix_timestamp(\"End_Time\"))\n",
    "\n",
    "# Calculate the duration (in seconds)\n",
    "merged_df = merged_df.withColumn(\"Duration\", col(\"End_Time_unix\") - col(\"Start_Time_unix\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for start_time\n",
    "merged_df = merged_df.withColumn('Hour_Of_Acc', hour(merged_df['Start_Time']))\n",
    "merged_df = merged_df.withColumn('Day_Of_Acc', date_format(merged_df['Start_Time'], 'EEEE'))\n",
    "merged_df = merged_df.withColumn('Month_Of_Acc', month(merged_df['Start_Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'when' function to assign temperature categories based on the bins\n",
    "temperature_category_column = when(merged_df['Temperature(F)'] <= 50, 'Cold') \\\n",
    "    .when((merged_df['Temperature(F)'] > 50) & (merged_df['Temperature(F)'] <= 75), 'Moderate') \\\n",
    "    .otherwise('Warm')\n",
    "\n",
    "# Add the temperature category column to the DataFrame\n",
    "merged_df = merged_df.withColumn('Temperature_Category', temperature_category_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Distance(mi)\" column from miles to meters\n",
    "merged_df = merged_df.withColumn(\"Distance(m)\", col(\"Distance(mi)\") * 1609.34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary or redundant columns\n",
    "\n",
    "# List of column names to drop\n",
    "columns_to_drop = ['Country', 'Start_Time', 'End_Time', 'Distance(mi)', 'Start_Time_unix', 'End_Time_unix']\n",
    "\n",
    "# Drop the unimportant columns\n",
    "merged_df = merged_df.drop(*columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---------+------------------+--------------------+--------------------+------------+----------+-----+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+--------+-----------+----------+------------+--------------------+-----------+\n",
      "|  ID| Source|Severity|Start_Lat|         Start_Lng|         Description|              Street|        City|    County|State|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Weather_Condition|Crossing|Railway|Station|Traffic_Calming|Traffic_Signal|Sunrise_Sunset|Duration|Hour_Of_Acc|Day_Of_Acc|Month_Of_Acc|Temperature_Category|Distance(m)|\n",
      "+----+-------+--------+---------+------------------+--------------------+--------------------+------------+----------+-----+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+--------+-----------+----------+------------+--------------------+-----------+\n",
      "| A-3|Source2|       2|39.063148|        -84.032608|Accident on OH-32...|      State Route 32|Williamsburg|  Clermont|   OH|          36.0|         33.3|      100.0|       29.67|          10.0|            SW|            3.5|         Overcast|   false|  false|  false|          false|          true|         Night|    1800|          6|    Monday|           2|                Cold|    16.0934|\n",
      "| A-4|Source2|       3|39.747753|-84.20558199999998|Accident on I-75 ...|              I-75 S|      Dayton|Montgomery|   OH|          35.1|         31.0|       96.0|       29.64|           9.0|            SW|            4.6|    Mostly Cloudy|   false|  false|  false|          false|         false|         Night|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "| A-5|Source2|       2|39.627781|        -84.188354|Accident on McEwe...|Miamisburg Center...|      Dayton|Montgomery|   OH|          36.0|         33.3|       89.0|       29.65|           6.0|            SW|            3.5|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "| A-6|Source2|       3| 40.10059|-82.92519399999998|Accident on I-270...|      Westerville Rd| Westerville|  Franklin|   OH|          37.9|         35.5|       97.0|       29.63|           7.0|           SSW|            3.5|       Light Rain|   false|  false|  false|          false|         false|           Day|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "| A-7|Source2|       2|39.758274|-84.23050699999997|Accident on Oakri...|      N Woodward Ave|      Dayton|Montgomery|   OH|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|    1800|          7|    Monday|           2|                Cold|        0.0|\n",
      "| A-8|Source2|       3|39.770382|        -84.194901|Accident on I-75 ...|           N Main St|      Dayton|Montgomery|   OH|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|         Overcast|   false|  false|  false|          false|         false|           Day|    1800|          7|    Monday|           2|                Cold|    16.0934|\n",
      "|A-10|Source2|       3| 40.10059|-82.92519399999998|Right hand should...|      Westerville Rd| Westerville|  Franklin|   OH|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-11|Source2|       3|39.952812|        -83.119293|Accident on I-270...|         Outerbelt S|    Columbus|  Franklin|   OH|          35.6|         30.7|       93.0|       29.64|           5.0|           WNW|            5.8|             Rain|    true|  false|  false|          false|         false|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-12|Source2|       3|39.932709|         -82.83091|One lane blocked ...|              I-70 E|Reynoldsburg|  Franklin|   OH|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|         false|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-14|Source2|       2| 39.79076|        -84.241547|Accident on Salem...|           Salem Ave|      Dayton|Montgomery|   OH|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|          true|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-15|Source2|       2|39.972038|        -82.913521|Accident on OH-16...|          E Broad St|    Columbus|  Franklin|   OH|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|       Light Rain|   false|  false|  false|          false|          true|           Day|    1800|          8|    Monday|           2|                Cold|    16.0934|\n",
      "|A-18|Source2|       2|39.752174|        -84.239952|Accident on Delph...|         Delphos Ave|      Dayton|Montgomery|   OH|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|    1800|          9|    Monday|           2|                Cold|        0.0|\n",
      "|A-19|Source2|       2|39.740669|        -84.184135|Accident on Stewa...|          Rubicon St|      Dayton|Montgomery|   OH|          37.4|         32.1|       93.0|       29.63|          10.0|           WSW|            6.9|         Overcast|    true|  false|  false|          false|          true|           Day|    1800|          9|    Monday|           2|                Cold|    16.0934|\n",
      "|A-20|Source2|       2|39.790703|        -84.244461|Accident on Hillc...|     W Hillcrest Ave|      Dayton|Montgomery|   OH|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|    1800|          9|    Monday|           2|                Cold|    16.0934|\n",
      "|A-21|Source2|       2|40.052509|-82.88233199999998|Accident on Brook...|        Brookhill Dr|    Columbus|  Franklin|   OH|          33.8|         29.6|      100.0|       29.62|           2.0|           NNW|            4.6|       Light Snow|   false|  false|  false|          false|         false|           Day|    1800|         10|    Monday|           2|                Cold|        0.0|\n",
      "|A-22|Source2|       2|39.773346|        -84.224686|Accident on Princ...|        Princeton Dr|      Dayton|Montgomery|   OH|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|    Mostly Cloudy|   false|  false|  false|          false|         false|           Day|    1800|         10|    Monday|           2|                Cold|        0.0|\n",
      "|A-23|Source2|       2|39.628288|        -84.226151|Accident on OH-74...|     Springboro Pike|  Miamisburg|Montgomery|   OH|          35.1|         28.6|       89.0|       29.65|           6.0|           WSW|            8.1|         Overcast|   false|  false|  false|          false|          true|           Day|    1800|         11|    Monday|           2|                Cold|    16.0934|\n",
      "|A-24|Source2|       3|40.023487|        -82.994888|Accident on I-71 ...|         North Fwy S|    Columbus|  Franklin|   OH|          37.0|         32.4|       96.0|       29.63|           8.0|          West|            5.8|         Overcast|   false|  false|  false|          false|         false|           Day|    2700|         12|    Monday|           2|                Cold|    16.0934|\n",
      "|A-25|Source2|       2|39.761379|-84.25921600000002|Accident on Hoove...|          Hoover Ave|      Dayton|Montgomery|   OH|          37.9|         30.9|       76.0|       29.62|          10.0|           WNW|           10.4|         Overcast|   false|  false|  false|          false|          true|           Day|    2700|         12|    Monday|           2|                Cold|    16.0934|\n",
      "|A-26|Source2|       2|40.158024|        -82.641762|Accident on Count...|   Sportsman Club Rd|   Johnstown|   Licking|   OH|          37.9|         34.4|      100.0|        29.6|           5.0|          West|            4.6|       Light Snow|   false|  false|  false|          false|         false|           Day|    1800|         12|    Monday|           2|                Cold|  2124.3288|\n",
      "+----+-------+--------+---------+------------------+--------------------+--------------------+------------+----------+-----+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+--------+-------+-------+---------------+--------------+--------------+--------+-----------+----------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Source',\n",
       " 'Severity',\n",
       " 'Start_Time',\n",
       " 'End_Time',\n",
       " 'Start_Lat',\n",
       " 'Start_Lng',\n",
       " 'Description',\n",
       " 'Street',\n",
       " 'City',\n",
       " 'County',\n",
       " 'State',\n",
       " 'Temperature(F)',\n",
       " 'Wind_Chill(F)',\n",
       " 'Humidity(%)',\n",
       " 'Pressure(in)',\n",
       " 'Visibility(mi)',\n",
       " 'Wind_Direction',\n",
       " 'Wind_Speed(mph)',\n",
       " 'Weather_Condition',\n",
       " 'Crossing',\n",
       " 'Railway',\n",
       " 'Station',\n",
       " 'Traffic_Calming',\n",
       " 'Traffic_Signal',\n",
       " 'Sunrise_Sunset',\n",
       " 'Distance(m)',\n",
       " 'Start_Time_unix',\n",
       " 'End_Time_unix',\n",
       " 'Duration']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics\n",
    "\n",
    "1. [Regression]➔ Predicting accident Duration as indicator of impact on\n",
    "traffic flow.\n",
    "\n",
    "2. [classification]➔ Predicting the severity of an accident based on the\n",
    "factors involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the EDA phase we will use the following features in our model:\n",
    "- Start_Lat\n",
    "- Start_Lng\n",
    "- Source\n",
    "- Duration\n",
    "- Hour_Of_Acc\n",
    "- Day_Of_Acc\n",
    "- Month_Of_Acc\n",
    "- Temperature\n",
    "- Distance\n",
    "- State\n",
    "- Crossing\n",
    "- Railway\n",
    "- Traffic_Calming\n",
    "- Traffic_Signal\n",
    "\n",
    "### Note: \n",
    "\n",
    "We execluded most of weather conditions as it does not affect on the severity as follows:  \n",
    "\n",
    "Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/07 16:34:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "|Start_Lat| Start_Lng| Source|Duration|Hour_Of_Acc|Day_Of_Acc|Month_Of_Acc|Temperature(F)|Distance(m)|State|        City|Crossing|Railway|Traffic_Calming|Traffic_Signal|Severity|\n",
      "+---------+----------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "|39.063148|-84.032608|Source2|    1800|          6|    Monday|           2|          36.0|    16.0934|   OH|Williamsburg|   false|  false|          false|          true|       2|\n",
      "+---------+----------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_to_keep = [\n",
    "    \"Start_Lat\", \"Start_Lng\",  'Source', \"Duration\", \"Hour_Of_Acc\", \"Day_Of_Acc\",\n",
    "    \"Month_Of_Acc\", \"Temperature(F)\", \"Distance(m)\", \"State\", \"City\", \"Crossing\",\n",
    "    \"Railway\", \"Traffic_Calming\", \"Traffic_Signal\", \"Severity\"\n",
    "]\n",
    "\n",
    "# Selecting only the desired columns\n",
    "data_df = merged_df.select(columns_to_keep)\n",
    "\n",
    "# Showing the resulting DataFrame\n",
    "data_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the string and boolean data into numerical for feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def convert_string_data(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    # Count the occurrences of each unique value\n",
    "    value_counts = df.groupBy(column_name).count()\n",
    "\n",
    "    # Sort by count in descending order and take the top 20\n",
    "    top_20_values = value_counts.orderBy(col('count').desc()).limit(20)\n",
    "\n",
    "    # Collect the top 20 values\n",
    "    top_20_values_list = top_20_values.select(column_name).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Create a new DataFrame to accumulate transformations\n",
    "    new_df = df\n",
    "\n",
    "    # Loop through each unique value and create a new column for each\n",
    "    for source in top_20_values_list:\n",
    "        new_column_name = source.replace(\" \", \"_\").lower() + \"_\" + column_name\n",
    "        # Accumulate transformations by using the new DataFrame\n",
    "        new_df = new_df.withColumn(new_column_name, when(col(column_name) == source, 1).otherwise(0))\n",
    "\n",
    "    # Drop the original column\n",
    "    new_df = new_df.drop(column_name)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def convert_boolean_data(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    new_df = df\n",
    "    new_df = new_df.withColumn(column_name, when(col(column_name), 1).otherwise(0))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "|Start_Lat| Start_Lng| Source|Duration|Hour_Of_Acc|Day_Of_Acc|Month_Of_Acc|Temperature(F)|Distance(m)|State|        City|Crossing|Railway|Traffic_Calming|Traffic_Signal|Severity|\n",
      "+---------+----------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "|39.063148|-84.032608|Source2|    1800|          6|    Monday|           2|          36.0|    16.0934|   OH|Williamsburg|       0|      0|              0|             1|       2|\n",
      "+---------+----------+-------+--------+-----------+----------+------------+--------------+-----------+-----+------------+--------+-------+---------------+--------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert boolean data\n",
    "boolean_cols = [ 'Crossing', 'Railway',  'Traffic_Calming', 'Traffic_Signal']  \n",
    "data_after_boolean = data_df\n",
    "for column in boolean_cols:\n",
    "    data_after_boolean = convert_boolean_data(data_after_boolean, column)\n",
    "data_after_boolean.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+------------+--------------+-----------+--------+-------+---------------+--------------+--------+--------------+--------------+--------------+-----------------+-------------------+--------------------+------------------+-----------------+-------------------+-----------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----------+----------------+------------+--------------+-----------+------------+------------+--------------+-----------+----------------+---------------+----------------+------------+--------------+------------+-------------+---------------+-----------+-----------------+------------------+\n",
      "|Start_Lat| Start_Lng|Duration|Hour_Of_Acc|Month_Of_Acc|Temperature(F)|Distance(m)|Crossing|Railway|Traffic_Calming|Traffic_Signal|Severity|source1_Source|source2_Source|source3_Source|friday_Day_Of_Acc|thursday_Day_Of_Acc|wednesday_Day_Of_Acc|tuesday_Day_Of_Acc|monday_Day_Of_Acc|saturday_Day_Of_Acc|sunday_Day_Of_Acc|ca_State|fl_State|tx_State|sc_State|ny_State|nc_State|va_State|pa_State|mn_State|or_State|tn_State|il_State|az_State|mi_State|la_State|ga_State|nj_State|md_State|oh_State|al_State|miami_City|los_angeles_City|houston_City|charlotte_City|dallas_City|orlando_City|raleigh_City|nashville_City|austin_City|baton_rouge_City|sacramento_City|minneapolis_City|atlanta_City|san_diego_City|phoenix_City|richmond_City|saint_paul_City|tucson_City|jacksonville_City|oklahoma_city_City|\n",
      "+---------+----------+--------+-----------+------------+--------------+-----------+--------+-------+---------------+--------------+--------+--------------+--------------+--------------+-----------------+-------------------+--------------------+------------------+-----------------+-------------------+-----------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----------+----------------+------------+--------------+-----------+------------+------------+--------------+-----------+----------------+---------------+----------------+------------+--------------+------------+-------------+---------------+-----------+-----------------+------------------+\n",
      "|39.063148|-84.032608|    1800|          6|           2|          36.0|    16.0934|       0|      0|              0|             1|       2|             0|             1|             0|                0|                  0|                   0|                 0|                1|                  0|                0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       1|       0|         0|               0|           0|             0|          0|           0|           0|             0|          0|               0|              0|               0|           0|             0|           0|            0|              0|          0|                0|                 0|\n",
      "+---------+----------+--------+-----------+------------+--------------+-----------+--------+-------+---------------+--------------+--------+--------------+--------------+--------------+-----------------+-------------------+--------------------+------------------+-----------------+-------------------+-----------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----------+----------------+------------+--------------+-----------+------------+------------+--------------+-----------+----------------+---------------+----------------+------------+--------------+------------+-------------+---------------+-----------+-----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert String data\n",
    "str_columns = [\"Source\", \"Day_Of_Acc\", \"State\", \"City\"]\n",
    "data_after_str = data_after_boolean\n",
    "for column in str_columns:\n",
    "    data_after_str = convert_string_data(data_after_str, column)\n",
    "\n",
    "data_after_str.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = data_after_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:====================>                                   (23 + 8) / 62]\r"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "from pyspark.sql.functions import rand\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Split the DataFrame into train and test\n",
    "train_df, test_df = new_df.randomSplit([train_ratio, test_ratio], seed=42)\n",
    "\n",
    "# Show the number of rows in each split\n",
    "# print(\"Number of rows in df:\",new_df.count())\n",
    "# print(\"Number of rows in train set:\", train_df.count())\n",
    "# print(\"Number of rows in test set:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'Severity' column: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:============================>                           (32 + 8) / 62]\r"
     ]
    }
   ],
   "source": [
    "# Get the index of 'price' column\n",
    "severity_index = new_df.columns.index('Severity')\n",
    "print(\"Index of 'Severity' column:\", severity_index)# Get the index of 'Severity' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================>                  (41 + 8) / 62]\r"
     ]
    }
   ],
   "source": [
    "# Define a function to extract features and label from each row\n",
    "def extract_features_label(row, severity_index):\n",
    "    features = row[:severity_index] + row[severity_index+1:]  # Extract all columns except the 'Severity' column\n",
    "    label = row[severity_index]  # Extract the 'Severity' column\n",
    "    return (features, label)\n",
    "transformed_rdd = train_df.rdd.map(lambda row: extract_features_label(row, severity_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33.183228,\n",
       "  -117.296371,\n",
       "  2673,\n",
       "  21,\n",
       "  12,\n",
       "  44.1,\n",
       "  16.0934,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0),\n",
       " 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from linear_regression_mr import linear_regression, predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_iterations = 100\n",
    "weights, cost_history = linear_regression(transformed_rdd, learning_rate, num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
